# Data-Transformation-using-Azure-Databricks
Indepth use case of Azure Data Factory, Databrick and PySpark

# What is PySpark?

Apache Spark is Python API for Apache Spark, an open source, distributed computing platform and collection of tools for real-time, massive data processing.
To build more scalable analyses and pipelines, PySpark is an useful language to learn if you're already familiar with Python and tools like Pandas.

Apache Spark is a computational engine that handles big data sets by processing them concurrently and in batches. PySpark was created to facilitate the integration of Python and Spark, which is written in Scala. PySpark makes use of the Py4j library to assist you in interacting with Resilient Distributed Datasets (RDDs), in addition to offering a Spark API.

-----

Popular library Py4J is included into PySpark and enables Python to interact dynamically with JVM (Java Virtual Machine) objects.

### PySpark features quite a few libraries:

- **PySparkSQL:** PySpark library for conducting SQL-like analysis on a significant volume of structured or semi-structured data With PySparkSQL, SQL queries are also an option.


- **MLlib:** A machine learning (ML) library wrapper for PySpark and Spark. Numerous machine learning methods for dimensionality reduction, collaborative filtering, clustering, classification, regression, and other tasks are supported by MLlib.


- **GraphFrames:** A graph processing toolkit that offers a collection of APIs for quickly and effectively doing graph analyses using PySpark core and PySparkSQL. It is geared toward distributed computing that is quick.

***Further Studies:***
- https://www.databricks.com/glossary/pyspark
- https://www.databricks.com/discover/introduction-to-data-analysis-workshop-series/intro-apache-spark
- https://www.dominodatalab.com/blog/considerations-for-using-spark-in-your-data-science-stack/
- https://www.databricks.com/session_na20/from-python-to-pyspark-and-back-again-unifying-single-host-and-distributed-deep-learning-with-maggy

# What is Azure Data Factory?
Azure Data Factory (ADF) is a cloud base service that primarily performs the complete (ETL) extract-transform-load process for ingesting, preparing, and transforming all your data at scale. The ADF service provides a code-free user interface for simple authoring and single-pane glass management. This allows users to build complex ETL processes for transforming data using dataflow or other computation services like Azure HDInsight, Azure Databricks, Azure Synapse Analytics, and Azure SQL Database.
![image.png](attachment:image.png)

# Project Architecture
![image-2.png](attachment:image-2.png)

# Appreciation:
Special Thanks to the following people for their continuous contribution to the Azure Community:
- [Maheer Basha Shaik](https://www.linkedin.com/in/maheer-basha-shaik-b50247102/)
- [Adam Marczak](https://www.linkedin.com/in/adam-marczak/)
- [Krish Naik](https://www.linkedin.com/in/naikkrish/)
- [Ramesh Retnasamy | Cloud Data Engineer/ Architect](https://www.linkedin.com/in/ramesh-retnasamy/)
- [Microsoft Learn](https://learn.microsoft.com/en-us/training/paths/data-engineer-azure-databricks/)

YouTube | Medium | Courses
- [PySPark Tutorial](https://www.youtube.com/watch?v=_C8kWso4ne4)
- [Adam Marczak - Azure for Everyone](https://www.youtube.com/@Azure4Everyone)
- [WafaStudies](https://www.youtube.com/@WafaStudies)
- [Azure Data Factory](https://www.udemy.com/course/learn-azure-data-factory-from-scratch/learn/lecture/23973042#overview)
**Google, YouTube & ChatgptðŸ˜‚**
